
# ğŸ¤– AI Research Assistant â€” LangJu Edition  
> æœ¬åœ° + äº‘ç«¯æ··åˆ AI Agentï¼šæ”¯æŒ Ollama(Qwen2) ä¸ OpenAI(GPT) æ¨¡å‹äº’æ¢ï¼›  
> é›†æˆ RAG æ–‡æ¡£æ£€ç´¢ã€çŸ­æœŸè®°å¿†ã€å·¥å…·è°ƒç”¨ã€Streamlit å‰ç«¯ã€FastAPI APIã€Docker éƒ¨ç½²ã€‚  

A **local + cloud hybrid AI Agent** powered by **Qwen2 (Ollama)** & **GPT (OpenAI)**.  
Includes **RAG**, **memory**, **tool use**, **Streamlit UI**, **FastAPI REST API**, and **Docker** support.

---

## âš™ï¸ Tech Stack
Python 3.11 Â· smolagents Â· LangChain Â· LiteLLM Â·  
Chroma / FAISS Â· SentenceTransformers Â· Streamlit Â· FastAPI Â· Docker  

---
## ğŸ§­ System Architecture (æ¶æ„æ€»è§ˆ)

```
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚           Streamlit å‰ç«¯ UI         â”‚
                â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
                â”‚  ğŸ“„ æ–‡ä»¶ä¸Šä¼  (st.file_uploader)     â”‚
                â”‚  ğŸ’¬ èŠå¤©è¾“å…¥æ¡† (st.chat_input)      â”‚
                â”‚  ğŸ“¤ å‘é€åˆ° FastAPI /chat æ¥å£       â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚          FastAPI åç«¯ (app.server) â”‚
               â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
               â”‚ ğŸ”§ æ¥å£1: /ingest (æ–‡æ¡£å…¥åº“)        â”‚
               â”‚ ğŸ”§ æ¥å£2: /chat (æ™ºèƒ½é—®ç­”)          â”‚
               â”‚                                    â”‚
               â”‚ å†…éƒ¨é€»è¾‘ï¼š                         â”‚
               â”‚ 1ï¸âƒ£ è¯»å– .env ä¸­çš„ OPENAI_API_KEY    â”‚
               â”‚ 2ï¸âƒ£ åŠ è½½ settings.yaml (æ¨¡å‹é…ç½®)     â”‚
               â”‚ 3ï¸âƒ£ è°ƒç”¨ agent_core.py å¤„ç†è¯·æ±‚       â”‚
               â”‚ 4ï¸âƒ£ è°ƒç”¨æ¨¡å‹ç”Ÿæˆå›ç­”                 â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚           agent_core.py æ¨¡å—        â”‚
               â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
               â”‚ ğŸ§  æ ¸å¿ƒåŠŸèƒ½ï¼š                       â”‚
               â”‚   - æ¨¡å‹è·¯ç”± (OpenAI / Ollama)     â”‚
               â”‚   - å‘é‡æœç´¢ (FAISS / Chroma)       â”‚
               â”‚   - RAG æ£€ç´¢å¢å¼º                    â”‚
               â”‚   - ç”Ÿæˆæœ€ç»ˆå“åº”                    â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚        æ¨¡å‹å±‚ (LLM Backend)        â”‚
               â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
               â”‚ ğŸ¤– å¯é€‰ï¼š                           â”‚
               â”‚  - OpenAI GPT (çº¿ä¸Šè°ƒç”¨)            â”‚
               â”‚  - Ollama æœ¬åœ°æ¨¡å‹ (Qwen2 / Llama)  â”‚
               â”‚                                    â”‚
               â”‚ ğŸŒ é…ç½®ï¼šsettings.yaml               â”‚
               â”‚ backend: openai / ollama            â”‚
               â”‚ model_id: gpt-4o-mini / qwen2:7b    â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
               â”‚          å‘é‡æ•°æ®åº“ (FAISS/Chroma) â”‚
               â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
               â”‚ ğŸ“š å­˜å‚¨ç”¨æˆ·ä¸Šä¼ çš„æ–‡æ¡£å‘é‡åµŒå…¥       â”‚
               â”‚ ğŸ” ç›¸ä¼¼åº¦æ£€ç´¢ä¾› RAG ä½¿ç”¨            â”‚
               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ”— Data & Call Flowï¼ˆè°ƒç”¨é“¾ï¼‰
1. **Streamlit** æ¥æ”¶è¾“å…¥/ä¸Šä¼  â†’ è°ƒç”¨ **FastAPI** `/chat` æˆ– `/ingest`ã€‚  
2. **FastAPI** è½¬äº¤ç»™ `agent_core.py`ï¼šè¯»å– `.env`ã€`settings.yaml`ã€è·¯ç”±åˆ°æ¨¡å‹ã€‚  
3. **agent_core** ä» `data/embeddings/`ï¼ˆFAISS/Chromaï¼‰æ£€ç´¢ç›¸å…³ç‰‡æ®µ â†’ ç»„ç»‡ä¸Šä¸‹æ–‡ã€‚  
4. **æ¨¡å‹ï¼ˆOpenAI/Ollamaï¼‰** ç”Ÿæˆç­”æ¡ˆ â†’ é€šè¿‡ **FastAPI** è¿”å› â†’ **Streamlit** å±•ç¤ºã€‚

### ğŸ“ File Mapï¼ˆå…³é”®æ–‡ä»¶æ˜ å°„ï¼‰
| Path | Purpose |
|------|---------|
| `app/server.py` | FastAPI æœåŠ¡å…¥å£ï¼ˆ/chat, /ingestï¼‰ |
| `app/main.py` | Streamlit å‰ç«¯å…¥å£ |
| `app/agent_core.py` | Agent é€»è¾‘ï¼šRAGã€å·¥å…·ã€æ¨¡å‹è°ƒç”¨ |
| `app/memory/vector_store.py` | FAISS/Chroma + è§£æä¸åˆ†å— |
| `app/config/settings.yaml` | é…ç½®æ¨¡å‹ä¸å‘é‡åº“ |
| `.env` | ç§å¯†å¯†é’¥ï¼ˆOPENAI_API_KEY ç­‰ï¼Œä¸æäº¤ Gitï¼‰ |

---

## ğŸš€ Run Locally (Streamlit Web UI)

```bash
# 1ï¸âƒ£ Create environment
python -m venv .venv && .\.venv\Scripts\activate   # (Windows PowerShell)


## ğŸ§‘â€ğŸ’» Developer quick start (Windows PowerShell)

If you're developing locally on Windows, use the included `run_dev.ps1` helper to create/activate the virtualenv, set PYTHONPATH to the repo root, and start Streamlit:

```powershell
# from repository root
.\run_dev.ps1            # start dev server
.\run_dev.ps1 -Reinstall # (optional) reinstall dependencies first
```

Manual steps (equivalent):

```powershell
# 2ï¸âƒ£ Install dependencies
pip install -r requirements.txt

# 3ï¸âƒ£ Run Streamlit interface
streamlit run app/main.py
```

ğŸ§© åŠŸèƒ½è¯´æ˜ï¼š
- ğŸ“„ ä¸Šä¼  PDF / TXT / DOCX æ–‡ä»¶ â†’ è‡ªåŠ¨åˆ†å—å¹¶åµŒå…¥å‘é‡æ•°æ®åº“  

Notes:
- If you see `ModuleNotFoundError: No module named 'app'`, make sure you ran the commands from the repository root and that the venv is activated (or use `run_dev.ps1`).
- If Streamlit errors with `UnicodeDecodeError` when reading `.py` files, convert the affected files to UTF-8 (VS Code: Reopen with Encoding â†’ Save with Encoding â†’ UTF-8).

## ğŸ›  Troubleshooting / FAQ

Q: I get `ModuleNotFoundError: No module named 'app'` when running Streamlit or tests. What do I do?

- Cause: Python's import search path (`sys.path`) doesn't include the repository root or you ran the script from the wrong folder.
- Quick fixes:
  - Run commands from the repository root.
  - Use the provided `run_dev.ps1` which sets `PYTHONPATH` for you.
  - Or run with the venv Python directly: `.\.venv\Scripts\python -m streamlit run app/main.py`.

Q: `UnicodeDecodeError: 'utf-8' codec can't decode bytes ...` reading my .py files.

- Cause: Some source files are saved in a non-UTF-8 encoding (e.g., GBK/ANSI on Windows).
- Fix:
  - In VS Code: Reopen with Encoding â†’ choose `Chinese (GBK)` to verify content, then Save with Encoding â†’ `UTF-8` (no BOM).
  - Or use the included script `scripts/ensure_utf8.py` to rewrite `.py` files under `app/` as UTF-8.

Q: `& .\.venv\Scripts\Activate.ps1` fails with a security error (script execution is disabled).

- Cause: PowerShell ExecutionPolicy blocks running scripts.
- Options:
  - Recommended: don't run Activate.ps1; use the venv Python directly: `.\.venv\Scripts\python -m pytest -q`.
  - Temporarily allow scripts in the current terminal: `Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope Process` then activate.

Q: CI fails installing packages like `faiss` or `sentence-transformers`.

- Cause: Some packages need native binaries or conda-based installs and fail on GitHub runners.
- Fixes:
  - Use a minimal test dependency set in CI (we install only pytest/pydantic/requests by default).
  - For full integration tests, use a dedicated runner with conda or an Ubuntu image that supports those libs, or mark heavy tests to run in a separate job.

## ğŸ” How to inspect failing CI runs (GitHub Actions)

1. Go to your repository on GitHub and click the "Actions" tab.
2. Select the workflow run that failed (they are grouped by workflow name and commit/PR).
3. Click the failing job (e.g., `test`, `checks`, or `integration`).
4. Expand the step that's marked with a red âŒ to see the console log and exact error output. The log contains the stdout/stderr from the runner.

Tips:
- Look at the first failing step; often an install step or a test command fails and subsequent steps are skipped.
- For dependency install failures, copy the failing command from the log and try to reproduce locally (use the same Python version/conda env).
- If a job is flaky (network or remote model download), re-run the workflow from the GitHub UI using "Re-run jobs".



- ğŸ’¬ ä¾§è¾¹æ é€‰æ‹©åç«¯ï¼ˆOllama æˆ– OpenAIï¼‰  
- ğŸ§  å…·å¤‡è®°å¿†åŠŸèƒ½ã€ä¸Šä¸‹æ–‡æ£€ç´¢ä¸å·¥å…·è°ƒç”¨  

---

## ğŸ§© FastAPI (REST API Server)

```bash
uvicorn app.server:app --reload --port 8000
```

**Chat endpoint**
```bash
curl -X POST http://localhost:8000/chat   -H "Content-Type: application/json"   -d '{"prompt":"Explain RAG in one sentence","backend":"openai"}'
```

**Ingest a document**
```bash
curl -X POST http://localhost:8000/ingest -F "file=@data/sample_docs/intro.txt"
```

ğŸ’¡ `/chat` â†’ è¾“å…¥ prompt è·å–æ™ºèƒ½å›å¤  
ğŸ’¡ `/ingest` â†’ ä¸Šä¼ æ–‡æ¡£è‡³å‘é‡åº“ï¼ˆChroma æˆ– FAISSï¼‰  

---

## ğŸ§  RAG / Vector DB Configuration

åœ¨ `app/config/settings.yaml` æ–‡ä»¶ä¸­é…ç½®å‘é‡æ•°æ®åº“å‚æ•°ï¼š
```yaml
rag:
  vector_db: faiss   # å¯é€‰ï¼šchroma æˆ– faiss
  collection_name: langju_docs
  top_k: 4
```

è¯´æ˜ï¼š
- **Chroma** â†’ å­˜å‚¨è·¯å¾„ï¼š`data/embeddings/chroma`
- **FAISS** â†’ å­˜å‚¨è·¯å¾„ï¼š`data/embeddings/faiss`
- é»˜è®¤ä½¿ç”¨ **SentenceTransformers** åµŒå…¥æ¨¡å‹ï¼š`all-MiniLM-L6-v2`
- æ”¯æŒ PDFã€DOCXã€TXT æ–‡æ¡£è§£æä¸åˆ†å—ï¼ˆ900 å­—ç¬¦ / 150 é‡å ï¼‰

---

## ğŸ” Switching Backend (Ollama â†” OpenAI)

| æ¨¡å¼ | åç«¯ | å¯åŠ¨æ–¹å¼ |
|------|------|-----------|
| æœ¬åœ°æ¨¡å‹ | Ollama (Qwen2:7b) | `ollama serve &` â†’ ä¾§è¾¹æ é€‰æ‹© Ollama |
| äº‘ç«¯æ¨¡å‹ | OpenAI GPT | è®¾ç½®ç¯å¢ƒå˜é‡ `OPENAI_API_KEY` |

PowerShell è®¾ç½®ç¤ºä¾‹ï¼š
```powershell
$env:OPENAI_API_KEY = "sk-xxxxxx"
```

---

## ğŸ³ Docker Deployment

```bash
# Build image
docker build -t langju/ai-agent-demo .

# Run container
docker run -p 8000:8000 -v ${PWD}/data:/app/data langju/ai-agent-demo
```

è¯´æ˜ï¼š
- Web UI: http://localhost:8501  
- API: http://localhost:8000  
- æ•°æ®æŒä¹…åŒ–ï¼šæŒ‚è½½ `data/` ç›®å½•ä¿å­˜åµŒå…¥å‘é‡  

---

## ğŸ§± Project Structure

```
ai-agent-demo/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py             # Streamlit Web UI
â”‚   â”œâ”€â”€ server.py           # FastAPI REST API
â”‚   â”œâ”€â”€ agent_core.py       # Agent orchestration logic
â”‚   â”œâ”€â”€ models/             # Model wrappers (Ollama/OpenAI)
â”‚   â”œâ”€â”€ memory/             # Vector store + memory modules
â”‚   â”œâ”€â”€ tools/              # Tool implementations
â”‚   â””â”€â”€ config/settings.yaml
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ sample_docs/
â”‚   â””â”€â”€ embeddings/
â”‚
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_agent.py
â”‚
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ RESUME_EN.md / RESUME_CN.md
â””â”€â”€ README.md
```

---

## ğŸ§  Features Summary
âœ… RAG (Retrieval-Augmented Generation)  
âœ… Hybrid backend: Ollama (local) & OpenAI (cloud)  
âœ… Streamlit UI + FastAPI RESTful API  
âœ… Embedding with SentenceTransformers  
âœ… Memory, logging, and vector caching  
âœ… One-command Docker deployment  

---

## âœ¨ Author
**Lang Ju (AI Engineer)**  
GitHub: [langju](https://github.com/langju) | LinkedIn: [langju](https://linkedin.com/in/langju)
